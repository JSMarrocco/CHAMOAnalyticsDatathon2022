{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "WCV-KJEVgxYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EC2UqeUQftho"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2bAzFd8f-cq",
        "outputId": "f8f0554c-e345-45ae-c44b-02d9c14c53a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_PATH = \"gdrive/MyDrive/ChamoAnalytics\""
      ],
      "metadata": {
        "id": "MBoat1NmgA9n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(f\"{DIR_PATH}/master_dataset.csv\", na_values=['-1'])"
      ],
      "metadata": {
        "id": "kAg5iqWsgB1_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "KEU22vVxvAF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.replace('.', np.nan)\n",
        "df[[\" us_bank_rate\"]] = df[[\" us_bank_rate\"]].apply(pd.to_numeric)\n",
        "df = df.rename(columns={' us_bank_rate': 'us_bank_rate'})"
      ],
      "metadata": {
        "id": "rgDu6Brieh2l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFIyISyXb371",
        "outputId": "999a2244-c561-40ca-b495-3c731dcc280a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date                            object\n",
              "rate                           float64\n",
              "sent_fin_us_uncertainty        float64\n",
              "sent_fin_us_litigious          float64\n",
              "sent_fin_us_strong_modal       float64\n",
              "sent_fin_us_weak_modal         float64\n",
              "sent_fin_us_constraining       float64\n",
              "sent_fin_us_optimistic         float64\n",
              "sent_fin_can_uncertainty       float64\n",
              "sent_fin_can_litigious         float64\n",
              "sent_fin_can_strong_modal      float64\n",
              "sent_fin_can_weak_modal        float64\n",
              "sent_fin_can_constraining      float64\n",
              "sent_fin_can_optimistic        float64\n",
              "ti_rate_rsi_14                 float64\n",
              "ti_rate_stochrsi_14            float64\n",
              "ti_rate_macd_12_26             float64\n",
              "ti_rate_adx_14                 float64\n",
              "ti_rate_williams_%R            float64\n",
              "ti_rate_cci                    float64\n",
              "ti_rate_atr                    float64\n",
              "ti_rate_utlimate_oscillator    float64\n",
              "ti_rate_roc                    float64\n",
              "interest_rate_can              float64\n",
              "index_W.BCPI                   float64\n",
              "index_W.BCNE                   float64\n",
              "index_W.ENER                   float64\n",
              "index_W.MTLS                   float64\n",
              "index_W.FOPR                   float64\n",
              "index_W.AGRI                   float64\n",
              "index_W.FISH                   float64\n",
              "index_WGTS.AGRI                float64\n",
              "index_WGTS.BRENT               float64\n",
              "index_WGTS.COAL                float64\n",
              "index_WGTS.FISH                float64\n",
              "index_WGTS.FOPR                float64\n",
              "index_WGTS.MTLS                float64\n",
              "index_WGTS.NATURALGAS          float64\n",
              "index_WGTS.WCC                 float64\n",
              "index_WGTS.WTI                 float64\n",
              "us_bank_rate                   float64\n",
              "sent_bert_us_fin               float64\n",
              "sent_bert_us_gen               float64\n",
              "sent_bert_can_fin_sentiment    float64\n",
              "sent_bert_can_gen_sentiment    float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt1388vDkr37",
        "outputId": "165dc901-ddbc-4131-cdb4-e2b028c310bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1028250"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['y_exp'] = np.full(df.shape[0], -1)\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    try:\n",
        "        two_weeks_from_current_date = datetime.datetime.strptime(row.date, \"%Y-%m-%d\") + datetime.timedelta(weeks=2)\n",
        "\n",
        "        futur_rate = list(df.loc[df['date'] == str(two_weeks_from_current_date.date())].rate)[0]\n",
        "        df.loc[i, 'y_exp' ] = futur_rate\n",
        "    except: \n",
        "        break\n",
        "\n",
        "df = df.loc[df.y_exp >= 0]"
      ],
      "metadata": {
        "id": "UmMOdEr7jOin"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "AP-pGyNcefqF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcVJ1V48_yIY",
        "outputId": "4c5edf65-2599-4c92-d67f-4c56601a2b1d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "307924"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_sequences(seq_size, obs, y):\n",
        "    x = []\n",
        "    y_out = []\n",
        "\n",
        "    for i in range(len(obs)-seq_size):\n",
        "        #print(i)\n",
        "        window = obs[i:(i+seq_size)]\n",
        "        after_window = y[i+seq_size - 1]\n",
        "        window = [x for x in window]\n",
        "        x.append(window)\n",
        "        y_out.append(after_window)\n",
        "        \n",
        "    return np.array(x), np.array(y_out)"
      ],
      "metadata": {
        "id": "h4GM-h18vBdk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test_val(df):\n",
        "    # Split train val test datasets\n",
        "    test_cutting_date = '2022-05-01'\n",
        "    val_cutting_date = '2022-02-01'\n",
        "    df_train = df[df['date'] < val_cutting_date]\n",
        "    df_val = df[(df['date'] >= val_cutting_date) &\n",
        "                (df['date'] < test_cutting_date)]\n",
        "    df_test = df[df['date'] >= test_cutting_date]\n",
        "    \n",
        "    return df_train, df_val, df_test"
      ],
      "metadata": {
        "id": "z-7UdlJ3z4Ov"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(df, cols_to_keep, seq_size):\n",
        "    df_train, df_val, df_test = split_train_test_val(df)\n",
        "\n",
        "\n",
        "    x_train = df_train[cols_to_keep].to_numpy()\n",
        "    y_train = df_train['y_exp'].to_numpy()\n",
        "\n",
        "    x_val = df_val[cols_to_keep].to_numpy()\n",
        "    y_val = df_val['y_exp'].to_numpy()\n",
        "\n",
        "    x_test = df_test[cols_to_keep].to_numpy()\n",
        "    y_test = df_test['y_exp'].to_numpy()\n",
        "\n",
        "    x_train, y_train = to_sequences(seq_size, x_train, y_train)\n",
        "    x_val, y_val = to_sequences(seq_size, x_val, y_val)\n",
        "    x_test, y_test = to_sequences(seq_size, x_test, y_test)\n",
        "\n",
        "    print(\"Train: {}\".format(x_train.shape))\n",
        "    print(\"Val: {}\".format(x_val.shape))\n",
        "    print(\"Test: {}\".format(x_test.shape))\n",
        "\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test"
      ],
      "metadata": {
        "id": "oT-aECF6j35I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "ONTm4Bv0g0Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "\n",
        "    return x + res"
      ],
      "metadata": {
        "id": "SfWPrBVagm1k"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1)(x)\n",
        "    \n",
        "    return keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "ocTQunDdPbXq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "nxstIb5-jZNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(x_train, y_train, x_val, y_val):\n",
        "    input_shape = x_train.shape[1:]\n",
        "\n",
        "    model = build_model(\n",
        "        input_shape,\n",
        "        head_size=256,\n",
        "        num_heads=4,\n",
        "        ff_dim=4,\n",
        "        num_transformer_blocks=4,\n",
        "        mlp_units=[128],\n",
        "        mlp_dropout=0.4,\n",
        "        dropout=0.25,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"mean_squared_error\",\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    )\n",
        "\n",
        "    callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "\n",
        "    model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=200,\n",
        "        batch_size=64,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "cEkxZjVhkwzY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating"
      ],
      "metadata": {
        "id": "DzCKZO9Ag189"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(df, cols_to_keep, seq_size, nb_trials=5):\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test = prepare_data(df, cols_to_keep, seq_size)\n",
        "\n",
        "    errors = []\n",
        "    models = []\n",
        "    smallest_error = None\n",
        "\n",
        "    for i in range(nb_trials):\n",
        "        model = train(x_train, y_train, x_val, y_val)\n",
        "        error = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "        if not smallest_error or smallest_error > error:\n",
        "            smallest_error = error\n",
        "\n",
        "        errors.append(error)\n",
        "        models.append(model)\n",
        "    \n",
        "    print('mean: ', np.mean(errors), 'var: ', np.var(errors))\n",
        "    print('smallest error: ', smallest_error)\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "vCdDHbM1pYjJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-5 of the features based on LSTM results"
      ],
      "metadata": {
        "id": "1TiqYHVf6aDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_keep = [\n",
        "    'interest_rate_can',\n",
        "    'index_WGTS.BRENT',\n",
        "    'rate',\n",
        "    'ti_rate_rsi_14',\n",
        "    'sent_fin_can_litigious',\n",
        "]\n",
        "\n",
        "seq_size = 20\n",
        "models = run(df, cols_to_keep, seq_size, nb_trials=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6YcGiuhEkKQ",
        "outputId": "75a9a72e-b465-466b-b93e-234b96c84ef9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (6490, 20, 5)\n",
            "Val: (43, 20, 5)\n",
            "Test: (101, 20, 5)\n",
            "Epoch 1/200\n",
            "102/102 [==============================] - 13s 22ms/step - loss: 21.3917 - val_loss: 0.3878\n",
            "Epoch 2/200\n",
            "102/102 [==============================] - 1s 14ms/step - loss: 5.1756 - val_loss: 0.8645\n",
            "Epoch 3/200\n",
            "102/102 [==============================] - 1s 15ms/step - loss: 2.1757 - val_loss: 0.5818\n",
            "Epoch 4/200\n",
            "102/102 [==============================] - 1s 15ms/step - loss: 1.6844 - val_loss: 0.4575\n",
            "Epoch 5/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.5196 - val_loss: 0.4084\n",
            "Epoch 6/200\n",
            "102/102 [==============================] - 1s 15ms/step - loss: 1.3752 - val_loss: 0.3268\n",
            "Epoch 7/200\n",
            "102/102 [==============================] - 1s 15ms/step - loss: 1.2270 - val_loss: 0.3371\n",
            "Epoch 8/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.1316 - val_loss: 0.2692\n",
            "Epoch 9/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.0430 - val_loss: 0.2391\n",
            "Epoch 10/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.9237 - val_loss: 0.2136\n",
            "Epoch 11/200\n",
            "102/102 [==============================] - 1s 15ms/step - loss: 0.8711 - val_loss: 0.3167\n",
            "Epoch 12/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.7634 - val_loss: 0.2677\n",
            "Epoch 13/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.6916 - val_loss: 0.2215\n",
            "Epoch 14/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.5863 - val_loss: 0.2479\n",
            "Epoch 15/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.5161 - val_loss: 0.1958\n",
            "Epoch 16/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.4508 - val_loss: 0.1624\n",
            "Epoch 17/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.3927 - val_loss: 0.1376\n",
            "Epoch 18/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.3142 - val_loss: 0.0987\n",
            "Epoch 19/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2575 - val_loss: 0.0563\n",
            "Epoch 20/200\n",
            "102/102 [==============================] - 1s 15ms/step - loss: 0.2186 - val_loss: 0.0564\n",
            "Epoch 21/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1809 - val_loss: 0.0486\n",
            "Epoch 22/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1639 - val_loss: 0.0392\n",
            "Epoch 23/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1460 - val_loss: 0.0278\n",
            "Epoch 24/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1300 - val_loss: 0.0211\n",
            "Epoch 25/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1189 - val_loss: 0.0174\n",
            "Epoch 26/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1127 - val_loss: 0.0114\n",
            "Epoch 27/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1053 - val_loss: 0.0095\n",
            "Epoch 28/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1054 - val_loss: 0.0090\n",
            "Epoch 29/200\n",
            "102/102 [==============================] - 1s 15ms/step - loss: 0.0976 - val_loss: 0.0131\n",
            "Epoch 30/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0930 - val_loss: 0.0044\n",
            "Epoch 31/200\n",
            "102/102 [==============================] - 1s 15ms/step - loss: 0.0936 - val_loss: 0.0060\n",
            "Epoch 32/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0885 - val_loss: 0.0029\n",
            "Epoch 33/200\n",
            "102/102 [==============================] - 1s 14ms/step - loss: 0.0883 - val_loss: 0.0054\n",
            "Epoch 34/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0830 - val_loss: 0.0028\n",
            "Epoch 35/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0799 - val_loss: 0.0037\n",
            "Epoch 36/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0791 - val_loss: 0.0025\n",
            "Epoch 37/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0787 - val_loss: 0.0025\n",
            "Epoch 38/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0761 - val_loss: 0.0036\n",
            "Epoch 39/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0732 - val_loss: 0.0069\n",
            "Epoch 40/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0732 - val_loss: 0.0020\n",
            "Epoch 41/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0694 - val_loss: 0.0029\n",
            "Epoch 42/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0678 - val_loss: 0.0029\n",
            "Epoch 43/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0662 - val_loss: 0.0036\n",
            "Epoch 44/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0638 - val_loss: 0.0026\n",
            "Epoch 45/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0656 - val_loss: 0.0032\n",
            "Epoch 46/200\n",
            "102/102 [==============================] - 2s 20ms/step - loss: 0.0626 - val_loss: 0.0038\n",
            "Epoch 47/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0631 - val_loss: 0.0046\n",
            "Epoch 48/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0610 - val_loss: 0.0027\n",
            "Epoch 49/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0606 - val_loss: 0.0184\n",
            "Epoch 50/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0593 - val_loss: 0.0130\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.0295\n",
            "Epoch 1/200\n",
            "102/102 [==============================] - 7s 26ms/step - loss: 32.8075 - val_loss: 0.4846\n",
            "Epoch 2/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 7.3187 - val_loss: 0.6393\n",
            "Epoch 3/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 2.8256 - val_loss: 0.4673\n",
            "Epoch 4/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.8747 - val_loss: 0.3823\n",
            "Epoch 5/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.5761 - val_loss: 0.2778\n",
            "Epoch 6/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.4105 - val_loss: 0.2283\n",
            "Epoch 7/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.2111 - val_loss: 0.2069\n",
            "Epoch 8/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.0749 - val_loss: 0.1978\n",
            "Epoch 9/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.9275 - val_loss: 0.1810\n",
            "Epoch 10/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.8829 - val_loss: 0.1645\n",
            "Epoch 11/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.7717 - val_loss: 0.1257\n",
            "Epoch 12/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.7238 - val_loss: 0.0966\n",
            "Epoch 13/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.6190 - val_loss: 0.1098\n",
            "Epoch 14/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.5832 - val_loss: 0.0957\n",
            "Epoch 15/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.5345 - val_loss: 0.1004\n",
            "Epoch 16/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.4884 - val_loss: 0.0599\n",
            "Epoch 17/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.4382 - val_loss: 0.0507\n",
            "Epoch 18/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.3982 - val_loss: 0.0517\n",
            "Epoch 19/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.3559 - val_loss: 0.0599\n",
            "Epoch 20/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.3159 - val_loss: 0.0540\n",
            "Epoch 21/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2882 - val_loss: 0.0429\n",
            "Epoch 22/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2631 - val_loss: 0.0362\n",
            "Epoch 23/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2438 - val_loss: 0.0403\n",
            "Epoch 24/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2334 - val_loss: 0.0234\n",
            "Epoch 25/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2028 - val_loss: 0.0278\n",
            "Epoch 26/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1937 - val_loss: 0.0205\n",
            "Epoch 27/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1786 - val_loss: 0.0256\n",
            "Epoch 28/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1666 - val_loss: 0.0203\n",
            "Epoch 29/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1557 - val_loss: 0.0083\n",
            "Epoch 30/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1469 - val_loss: 0.0123\n",
            "Epoch 31/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1391 - val_loss: 0.0136\n",
            "Epoch 32/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1360 - val_loss: 0.0117\n",
            "Epoch 33/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1277 - val_loss: 0.0072\n",
            "Epoch 34/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1235 - val_loss: 0.0078\n",
            "Epoch 35/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1174 - val_loss: 0.0079\n",
            "Epoch 36/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1130 - val_loss: 0.0045\n",
            "Epoch 37/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1105 - val_loss: 0.0046\n",
            "Epoch 38/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1073 - val_loss: 0.0026\n",
            "Epoch 39/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1012 - val_loss: 0.0067\n",
            "Epoch 40/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0986 - val_loss: 0.0050\n",
            "Epoch 41/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0941 - val_loss: 0.0022\n",
            "Epoch 42/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0941 - val_loss: 0.0026\n",
            "Epoch 43/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0914 - val_loss: 0.0036\n",
            "Epoch 44/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0895 - val_loss: 0.0019\n",
            "Epoch 45/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0855 - val_loss: 0.0020\n",
            "Epoch 46/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0841 - val_loss: 0.0020\n",
            "Epoch 47/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0850 - val_loss: 0.0022\n",
            "Epoch 48/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0824 - val_loss: 0.0022\n",
            "Epoch 49/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0800 - val_loss: 0.0025\n",
            "Epoch 50/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0779 - val_loss: 0.0024\n",
            "Epoch 51/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0783 - val_loss: 0.0028\n",
            "Epoch 52/200\n",
            "102/102 [==============================] - 1s 15ms/step - loss: 0.0757 - val_loss: 0.0025\n",
            "Epoch 53/200\n",
            "102/102 [==============================] - 1s 15ms/step - loss: 0.0734 - val_loss: 0.0058\n",
            "Epoch 54/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0711 - val_loss: 0.0040\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0374\n",
            "Epoch 1/200\n",
            "102/102 [==============================] - 5s 20ms/step - loss: 24.9589 - val_loss: 0.9174\n",
            "Epoch 2/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 6.1506 - val_loss: 1.7907\n",
            "Epoch 3/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 3.0025 - val_loss: 0.6734\n",
            "Epoch 4/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 2.2691 - val_loss: 0.5153\n",
            "Epoch 5/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.9719 - val_loss: 0.3404\n",
            "Epoch 6/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.7602 - val_loss: 0.2654\n",
            "Epoch 7/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.5955 - val_loss: 0.2702\n",
            "Epoch 8/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.4591 - val_loss: 0.2360\n",
            "Epoch 9/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.3350 - val_loss: 0.2179\n",
            "Epoch 10/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.2152 - val_loss: 0.2325\n",
            "Epoch 11/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.1005 - val_loss: 0.2105\n",
            "Epoch 12/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.9849 - val_loss: 0.2097\n",
            "Epoch 13/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.8679 - val_loss: 0.1896\n",
            "Epoch 14/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.7829 - val_loss: 0.2069\n",
            "Epoch 15/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.7120 - val_loss: 0.2097\n",
            "Epoch 16/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.6087 - val_loss: 0.1944\n",
            "Epoch 17/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.5378 - val_loss: 0.1801\n",
            "Epoch 18/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.4652 - val_loss: 0.2027\n",
            "Epoch 19/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.4003 - val_loss: 0.1612\n",
            "Epoch 20/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.3302 - val_loss: 0.1540\n",
            "Epoch 21/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2984 - val_loss: 0.1201\n",
            "Epoch 22/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2476 - val_loss: 0.1097\n",
            "Epoch 23/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2265 - val_loss: 0.0898\n",
            "Epoch 24/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2021 - val_loss: 0.0641\n",
            "Epoch 25/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1780 - val_loss: 0.0702\n",
            "Epoch 26/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.1673 - val_loss: 0.0418\n",
            "Epoch 27/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1559 - val_loss: 0.0394\n",
            "Epoch 28/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1381 - val_loss: 0.0260\n",
            "Epoch 29/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1325 - val_loss: 0.0275\n",
            "Epoch 30/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1223 - val_loss: 0.0260\n",
            "Epoch 31/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1187 - val_loss: 0.0104\n",
            "Epoch 32/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1142 - val_loss: 0.0119\n",
            "Epoch 33/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.1068 - val_loss: 0.0072\n",
            "Epoch 34/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1037 - val_loss: 0.0068\n",
            "Epoch 35/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0982 - val_loss: 0.0049\n",
            "Epoch 36/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0976 - val_loss: 0.0056\n",
            "Epoch 37/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0953 - val_loss: 0.0050\n",
            "Epoch 38/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0891 - val_loss: 0.0040\n",
            "Epoch 39/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0891 - val_loss: 0.0037\n",
            "Epoch 40/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0872 - val_loss: 0.0033\n",
            "Epoch 41/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0834 - val_loss: 0.0028\n",
            "Epoch 42/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0781 - val_loss: 0.0041\n",
            "Epoch 43/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0778 - val_loss: 0.0024\n",
            "Epoch 44/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0753 - val_loss: 0.0035\n",
            "Epoch 45/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0754 - val_loss: 0.0039\n",
            "Epoch 46/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0732 - val_loss: 0.0114\n",
            "Epoch 47/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0717 - val_loss: 0.0102\n",
            "Epoch 48/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0681 - val_loss: 0.0078\n",
            "Epoch 49/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0654 - val_loss: 0.0092\n",
            "Epoch 50/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0657 - val_loss: 0.0118\n",
            "Epoch 51/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0648 - val_loss: 0.0068\n",
            "Epoch 52/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0632 - val_loss: 0.0058\n",
            "Epoch 53/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0634 - val_loss: 0.0139\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0337\n",
            "Epoch 1/200\n",
            "102/102 [==============================] - 5s 20ms/step - loss: 16.4877 - val_loss: 0.7895\n",
            "Epoch 2/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 4.4856 - val_loss: 0.9238\n",
            "Epoch 3/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 2.6223 - val_loss: 0.5078\n",
            "Epoch 4/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 2.1118 - val_loss: 0.3336\n",
            "Epoch 5/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 1.8542 - val_loss: 0.2489\n",
            "Epoch 6/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.6775 - val_loss: 0.2319\n",
            "Epoch 7/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.4714 - val_loss: 0.2240\n",
            "Epoch 8/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.4238 - val_loss: 0.2184\n",
            "Epoch 9/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.2642 - val_loss: 0.1890\n",
            "Epoch 10/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.1813 - val_loss: 0.1935\n",
            "Epoch 11/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.0515 - val_loss: 0.1567\n",
            "Epoch 12/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.9429 - val_loss: 0.1756\n",
            "Epoch 13/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.8581 - val_loss: 0.1215\n",
            "Epoch 14/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.8078 - val_loss: 0.1551\n",
            "Epoch 15/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.7103 - val_loss: 0.1330\n",
            "Epoch 16/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.6166 - val_loss: 0.0828\n",
            "Epoch 17/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.5643 - val_loss: 0.0751\n",
            "Epoch 18/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.4924 - val_loss: 0.0809\n",
            "Epoch 19/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.4270 - val_loss: 0.0781\n",
            "Epoch 20/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.3785 - val_loss: 0.0536\n",
            "Epoch 21/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.3250 - val_loss: 0.0457\n",
            "Epoch 22/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2731 - val_loss: 0.0328\n",
            "Epoch 23/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2392 - val_loss: 0.0322\n",
            "Epoch 24/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.2033 - val_loss: 0.0191\n",
            "Epoch 25/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1790 - val_loss: 0.0207\n",
            "Epoch 26/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1617 - val_loss: 0.0126\n",
            "Epoch 27/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1497 - val_loss: 0.0153\n",
            "Epoch 28/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1378 - val_loss: 0.0126\n",
            "Epoch 29/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1307 - val_loss: 0.0058\n",
            "Epoch 30/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1215 - val_loss: 0.0059\n",
            "Epoch 31/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1161 - val_loss: 0.0045\n",
            "Epoch 32/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.1067 - val_loss: 0.0021\n",
            "Epoch 33/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1009 - val_loss: 0.0028\n",
            "Epoch 34/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0957 - val_loss: 0.0019\n",
            "Epoch 35/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0957 - val_loss: 0.0017\n",
            "Epoch 36/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0939 - val_loss: 0.0012\n",
            "Epoch 37/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0879 - val_loss: 0.0014\n",
            "Epoch 38/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0863 - val_loss: 0.0017\n",
            "Epoch 39/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0844 - val_loss: 0.0012\n",
            "Epoch 40/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0814 - val_loss: 0.0030\n",
            "Epoch 41/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0804 - val_loss: 0.0058\n",
            "Epoch 42/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0775 - val_loss: 0.0096\n",
            "Epoch 43/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0775 - val_loss: 0.0085\n",
            "Epoch 44/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0736 - val_loss: 0.0080\n",
            "Epoch 45/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0726 - val_loss: 0.0094\n",
            "Epoch 46/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0722 - val_loss: 0.0160\n",
            "Epoch 47/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0704 - val_loss: 0.0096\n",
            "Epoch 48/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0691 - val_loss: 0.0128\n",
            "Epoch 49/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0682 - val_loss: 0.0116\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0268\n",
            "Epoch 1/200\n",
            "102/102 [==============================] - 6s 20ms/step - loss: 16.1922 - val_loss: 0.5358\n",
            "Epoch 2/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 4.2049 - val_loss: 0.7904\n",
            "Epoch 3/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 2.0445 - val_loss: 0.5733\n",
            "Epoch 4/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 1.5688 - val_loss: 0.4161\n",
            "Epoch 5/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.3892 - val_loss: 0.3229\n",
            "Epoch 6/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 1.2717 - val_loss: 0.3022\n",
            "Epoch 7/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.1498 - val_loss: 0.2342\n",
            "Epoch 8/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 1.0424 - val_loss: 0.2537\n",
            "Epoch 9/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.9768 - val_loss: 0.2443\n",
            "Epoch 10/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.8089 - val_loss: 0.2453\n",
            "Epoch 11/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.7617 - val_loss: 0.2277\n",
            "Epoch 12/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.6681 - val_loss: 0.2201\n",
            "Epoch 13/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.5929 - val_loss: 0.2306\n",
            "Epoch 14/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.5144 - val_loss: 0.1993\n",
            "Epoch 15/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.4373 - val_loss: 0.1698\n",
            "Epoch 16/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.3877 - val_loss: 0.1898\n",
            "Epoch 17/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.3214 - val_loss: 0.1471\n",
            "Epoch 18/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.2669 - val_loss: 0.1402\n",
            "Epoch 19/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.2438 - val_loss: 0.0852\n",
            "Epoch 20/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.2114 - val_loss: 0.0697\n",
            "Epoch 21/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1848 - val_loss: 0.0503\n",
            "Epoch 22/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1652 - val_loss: 0.0322\n",
            "Epoch 23/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.1480 - val_loss: 0.0218\n",
            "Epoch 24/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.1366 - val_loss: 0.0188\n",
            "Epoch 25/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.1284 - val_loss: 0.0131\n",
            "Epoch 26/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.1210 - val_loss: 0.0081\n",
            "Epoch 27/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.1134 - val_loss: 0.0067\n",
            "Epoch 28/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.1076 - val_loss: 0.0059\n",
            "Epoch 29/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.1011 - val_loss: 0.0024\n",
            "Epoch 30/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.1003 - val_loss: 0.0024\n",
            "Epoch 31/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0940 - val_loss: 0.0033\n",
            "Epoch 32/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0868 - val_loss: 0.0019\n",
            "Epoch 33/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0871 - val_loss: 0.0036\n",
            "Epoch 34/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0840 - val_loss: 0.0056\n",
            "Epoch 35/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0786 - val_loss: 0.0088\n",
            "Epoch 36/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0787 - val_loss: 0.0030\n",
            "Epoch 37/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0749 - val_loss: 0.0023\n",
            "Epoch 38/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0768 - val_loss: 0.0082\n",
            "Epoch 39/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0721 - val_loss: 0.0080\n",
            "Epoch 40/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0721 - val_loss: 0.0016\n",
            "Epoch 41/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0702 - val_loss: 0.0133\n",
            "Epoch 42/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0677 - val_loss: 0.0076\n",
            "Epoch 43/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0662 - val_loss: 0.0062\n",
            "Epoch 44/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0665 - val_loss: 0.0166\n",
            "Epoch 45/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0651 - val_loss: 0.0068\n",
            "Epoch 46/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0647 - val_loss: 0.0140\n",
            "Epoch 47/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0634 - val_loss: 0.0089\n",
            "Epoch 48/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0606 - val_loss: 0.0095\n",
            "Epoch 49/200\n",
            "102/102 [==============================] - 2s 15ms/step - loss: 0.0605 - val_loss: 0.0167\n",
            "Epoch 50/200\n",
            "102/102 [==============================] - 2s 16ms/step - loss: 0.0603 - val_loss: 0.0170\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0208\n",
            "mean:  0.029629971459507944 var:  3.234556760442542e-05\n",
            "smallest error:  0.020823806524276733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, model in enumerate(models):\n",
        "    model.save(f\"{DIR_PATH}/transformer_models/model_{i}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdXfo9kLGwuo",
        "outputId": "c0f7e738-658a-44ca-b6f1-8a23227d6c4c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}